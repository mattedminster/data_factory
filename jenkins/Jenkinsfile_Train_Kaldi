pipeline {
    agent{
           docker { image 'data_factory' 
           args '-v /home/robot/data_factory/youtube_downloads:/youtube_downloads:rw -v /home/robot/data_factory/egs:/opt/kaldi/egs:rw '}
    }
    parameters {
        string(name: 'num_of_jobs', defaultValue: '14', description: 'How many jobs do you want to be sent out (can\'t be more than number of speakers you have)')
        string(name: 'monophone_max', defaultValue: '14', description: 'How much of the data set should we sample for the monophone training?')
    }

    stages {
        stage('Generate Files') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && python3 generate_files.py"
            }
        }
        stage('Cut') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && ./cut.sh"
            }
        }
        stage('Filter Lexicon') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && python3 filter_dict.py"
            }
        }
        stage('Generate spk2utt') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && utils/fix_data_dir.sh data/train"
            }
        }
        stage('Nonsilence Phones') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe/data/local/lang && ./nonsilence_phones.sh"
            }
        }
        stage('Optional Silence') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe/data/local/lang && ./optional_silence.sh"
            }
        }
        stage('Create data/lang') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && ./prepare_lang.sh"
            }
        }
        stage('Extract MFCC') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && ./extract_mffc.sh"
            }
        }
        stage('Monophone Training') {
            steps {
                sh "cd /opt/kaldi/egs/horseshoe && utils/subset_data_dir.sh --first data/train ${params.monophone_max} data/train_10k
            }
        }
    }
}